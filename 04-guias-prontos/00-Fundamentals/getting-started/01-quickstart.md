# Quick Start: RAG Fundamentals

**Tempo estimado:** 15-30 minutos
**N√≠vel:** Iniciante
**Pr√©-requisitos:** Python 3.8+, OpenAI API key (opcional)

## Objetivo
Aprender os conceitos fundamentais de RAG e criar seu primeiro sistema b√°sico em 15 minutos.

## O que √© RAG?
RAG (Retrieval-Augmented Generation) combina:
- **Mem√≥ria Param√©trica** (modelos pr√©-treinados) - conhecimento geral
- **Mem√≥ria N√£o-Param√©trica** (√≠ndices vetoriais) - conhecimento externo

```
Usu√°rio Pergunta ‚Üí Busca Relevante ‚Üí LLM Responde com Contexto
```

## Passo a Passo

### Passo 1: Instalar Depend√™ncias
```bash
pip install langchain openai chromadb
```

### Passo 2: Entender a Arquitetura
RAG tem 2 fases:

**FASE 1: Indexing (uma vez)**
1. Load documents
2. Split em chunks
3. Generate embeddings
4. Store no vector DB

**FASE 2: Query (sempre)**
1. Embed user query
2. Search similar chunks
3. Generate com contexto

### Passo 3: Primeiro Exemplo (5 min)

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA

# 1. Setup (configure sua API key)
# export OPENAI_API_KEY="sua-key-aqui"
embeddings = OpenAIEmbeddings()
llm = OpenAI(temperature=0)

# 2. Create vector store (indexing)
texts = [
    "RAG √© uma t√©cnica que combina busca e gera√ß√£o",
    "RAG usa mem√≥ria param√©trica e n√£o-param√©trica",
    "Lewis et al. (2020) introduziu RAG"
]
vectorstore = Chroma.from_texts(texts, embeddings)

# 3. Create QA chain
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever()
)

# 4. Query
answer = qa.run("O que √© RAG?")
print(answer)
```

### Passo 4: Exemplo com PDF

```python
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 1. Load PDF
loader = PyPDFLoader("documento.pdf")
pages = loader.load()

# 2. Split em chunks
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = splitter.split_documents(pages)

# 3. Index
vectorstore = Chroma.from_documents(chunks, embeddings)

# 4. Query
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever()
)
answer = qa.run("Qual o tema principal?")
print(answer)
```

### Passo 5: Compreender os Par√¢metros

**Chunking (divis√£o de texto):**
- `chunk_size=1000` - Tamanho ideal para equil√≠brio
- `chunk_overlap=200` - Sobreposi√ß√£o preserva contexto

**Retrieval (busca):**
- `k=2-5` - N√∫mero de documentos a recuperar
- `temperature=0` - Determin√≠stico para factualidade

## Exemplo Completo Test√°vel

```python
#!/usr/bin/env python3
"""
RAG Quick Start - Exemplo b√°sico funcional
"""

from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import HuggingFaceHub

# 1. Setup (sem API key necess√°ria)
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
llm = HuggingFaceHub(
    repo_id="google/flan-t5-large",
    model_kwargs={"temperature": 0.1}
)

# 2. Texto de exemplo
texts = [
    "RAG combina mem√≥ria param√©trica e n√£o-param√©trica",
    "Mem√≥ria param√©trica √© o conhecimento dos LLMs pr√©-treinados",
    "Mem√≥ria n√£o-param√©trica s√£o os √≠ndices vetoriais externos",
    "RAG reduz hallucinations e melhora factualidade"
]

# 3. Index
vectorstore = FAISS.from_texts(texts, embeddings)

# 4. Query
retriever = vectorstore.as_retriever(search_k=2)
docs = retriever.get_relevant_documents("O que √© RAG?")

# 5. Generate
context = "\n".join([doc.page_content for doc in docs])
prompt = f"Com base no contexto: {context}\n\nPergunta: O que √© RAG?\nResposta:"
answer = llm(prompt)

print(f"Contexto encontrado:")
for i, doc in enumerate(docs, 1):
    print(f"{i}. {doc.page_content}")

print(f"\nResposta: {answer}")
```

## Quando Usar RAG?

### ‚úÖ USE RAG se:
- Precisa de knowledge up-to-date
- Dados mudam frequentemente
- Precisa de citations/explicabilidade
- Volume de dados √© grande
- Custo de fine-tuning √© alto

### ‚ùå N√ÉO USE se:
- Dom√≠nio restrito e est√°tico
- Precisa de performance m√°xima
- Tem budget para fine-tuning
- Queries sempre similares

## Pr√≥ximos Passos

- üìñ **Tutorial Intermedi√°rio:** [LangChain vs LlamaIndex](../tutorials/02-intermediate.md)
- üíª **Tutoriais Pr√°ticos:** [Tutoriais](../tutorials/)
- üìö **Exemplos Completos:** [Code Examples](../code-examples/)
- üîß **Troubleshooting:** [Problemas Comuns](../troubleshooting/common-issues.md)

## Recursos

- üìÑ **Paper Original:** Lewis et al. (2020) - https://arxiv.org/abs/2005.11401
- üìñ **LangChain Docs:** https://docs.langchain.com/oss/python/langchain/rag
- ü¶ô **LlamaIndex:** https://developers.llamaindex.ai/
- üéØ **Compara√ß√£o Frameworks:** [Guia 10](../10-Frameworks-Tools/README.md)

## Problemas Comuns

### Erro: API Key n√£o configurada
**Solu√ß√£o:** Configure a vari√°vel de ambiente
```bash
export OPENAI_API_KEY="sua-key-aqui"
```

### Erro: ImportError
**Solu√ß√£o:** Instalar depend√™ncias
```bash
pip install --upgrade langchain chromadb
```

### Resposta sem sentido
**Solu√ß√µes:**
1. Verificar `chunk_size=1000` e `overlap=200`
2. Ajustar `k=2-5` (mais contexto)
3. Usar embeddings melhores
4. Prompts mais espec√≠ficos

---

**Pr√≥ximo:** [Tutorial Intermedi√°rio: Comparando Abordagens](../tutorials/02-intermediate.md)
