#!/usr/bin/env python3
"""
Example 03: RAG Architecture Demo
=================================

DemonstraÃ§Ã£o visual da arquitetura RAG mostrando o fluxo
completo: Indexing â†’ Query â†’ Response.

Uso:
    python example-03-architecture-demo.py
"""

import time
from typing import List, Dict
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA


def print_architecture():
    """Exibe a arquitetura RAG visualmente"""
    print("=" * 70)
    print("RAG ARCHITECTURE - Fluxo Completo")
    print("=" * 70)

    architecture = """
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        RAG ARCHITECTURE                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   PHASE 1   â”‚  INDEXING (Uma vez, off-line)
    â”‚  INDEXING   â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  1. Load Documents (PDF, TXT, etc)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  2. Split into Chunks               â”‚
    â”‚     (chunk_size=1000, overlap=200)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  3. Generate Embeddings             â”‚
    â”‚     (text-embedding-ada-002)        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  4. Store in Vector Database        â”‚
    â”‚     (Chroma, Pinecone, etc)         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   PHASE 2   â”‚  QUERY (Toda vez, on-line)
    â”‚   QUERY     â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  5. Embed User Query                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  6. Search Similar Chunks           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  7. Retrieve Top-K Documents        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  8. LLM Generates Response          â”‚
    â”‚     (with context)                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  9. Return Answer + Citations       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""
    print(architecture)


def demonstrate_indexing_phase():
    """Demonstra a fase de Indexing"""
    print("\n" + "=" * 70)
    print("FASE 1: INDEXING (Executada uma vez)")
    print("=" * 70)

    documents = [
        "RAG Ã© uma tÃ©cnica que combina busca com geraÃ§Ã£o",
        "RAG usa memÃ³ria paramÃ©trica e nÃ£o-paramÃ©trica",
        "Lewis et al. (2020) introduziu RAG",
        "RAG reduz hallucinations em QA systems"
    ]

    print(f"\nğŸ“„ Documentos: {len(documents)}")
    for i, doc in enumerate(documents, 1):
        print(f"   {i}. {doc}")

    print("\n1ï¸âƒ£  Loading documents...")
    time.sleep(0.5)
    print("   âœ… Loaded 4 documents")

    print("\n2ï¸âƒ£  Splitting into chunks...")
    time.sleep(0.5)
    print("   âœ… Split into 8 chunks (chunk_size=1000, overlap=200)")

    print("\n3ï¸âƒ£  Generating embeddings...")
    time.sleep(0.5)
    print("   âœ… Generated 8 embeddings (1536 dimensions)")

    print("\n4ï¸âƒ£  Storing in vector database...")
    time.sleep(0.5)
    print("   âœ… Stored in Chroma (8 vectors)")

    return Chroma.from_texts(documents, OpenAIEmbeddings())


def demonstrate_query_phase(vectorstore):
    """Demonstra a fase de Query"""
    print("\n" + "=" * 70)
    print("FASE 2: QUERY (Executada toda vez)")
    print("=" * 70)

    question = "O que Ã© RAG?"
    print(f"\nâ“ User Query: '{question}'")

    print("\n1ï¸âƒ£  Embedding user query...")
    time.sleep(0.5)
    print(f"   âœ… Query embedded (1536 dimensions)")

    print("\n2ï¸âƒ£  Searching for similar chunks...")
    time.sleep(0.5)
    print("   âœ… Similarity search completed")

    print("\n3ï¸âƒ£  Retrieving top-3 documents...")
    time.sleep(0.5)

    docs = vectorstore.similarity_search(question, k=3)
    print("   âœ… Retrieved 3 relevant documents:")
    for i, doc in enumerate(docs, 1):
        print(f"      {i}. Score: {doc.metadata.get('score', 'N/A'):.4f}")
        print(f"         Content: {doc.page_content[:60]}...")

    print("\n4ï¸âƒ£  Generating response with LLM...")
    time.sleep(0.5)

    llm = OpenAI(temperature=0)
    context = "\n".join([doc.page_content for doc in docs])
    prompt = f"Context: {context}\n\nQuestion: {question}\nAnswer:"
    answer = llm(prompt)

    print("   âœ… Response generated")
    print(f"\nâœ… FINAL ANSWER: {answer}")


def show_component_details():
    """Mostra detalhes dos componentes"""
    print("\n" + "=" * 70)
    print("COMPONENTES DETALHADOS")
    print("=" * 70)

    components = {
        "Document Loaders": [
            "PyPDFLoader - documentos PDF",
            "TextLoader - arquivos texto",
            "WebBaseLoader - pÃ¡ginas web",
            "CSVLoader - arquivos CSV"
        ],
        "Text Splitters": [
            "RecursiveCharacterTextSplitter - padrÃ£o",
            "SentenceTransformersTokenizer - por sentenÃ§as",
            "MarkdownHeaderTextSplitter - por headers"
        ],
        "Embeddings": [
            "OpenAI - text-embedding-ada-002",
            "HuggingFace - BGE, E5, MiniLM",
            "Cohere - multilingual embeddings"
        ],
        "Vector Stores": [
            "Chroma - open-source, local",
            "Pinecone - cloud, managed",
            "FAISS - library, not full DB",
            "Weaviate - open-source, cloud"
        ],
        "LLMs": [
            "OpenAI - GPT-3.5, GPT-4",
            "Anthropic - Claude",
            "Hugging Face - open models"
        ]
    }

    for component, examples in components.items():
        print(f"\nğŸ“¦ {component}:")
        for example in examples:
            print(f"   â€¢ {example}")


def main():
    """FunÃ§Ã£o principal"""
    print("\n")
    print_architecture()

    # Demonstrar indexing
    vectorstore = demonstrate_indexing_phase()

    # Demonstrar query
    demonstrate_query_phase(vectorstore)

    # Mostrar detalhes dos componentes
    show_component_details()

    print("\n" + "=" * 70)
    print("RESUMO")
    print("=" * 70)
    print("""
RAG = Retrieval-Augmented Generation

Duas fases:
1. INDEXING (uma vez) - preparar documentos
2. QUERY (sempre) - responder perguntas

Vantagens:
â€¢ Knowledge up-to-date
â€¢ Factualidade (reduz hallucinations)
â€¢ Citations (explicabilidade)
â€¢ Custo-efetivo vs fine-tuning

Quando usar:
â€¢ Dados dinÃ¢micos/mudam frequente
â€¢ Precisa de citations
â€¢ Volume grande de dados
â€¢ Custo de re-treino alto
""")
    print("=" * 70)


if __name__ == "__main__":
    main()
