# Quick Start: Embedding Models

**Tempo estimado:** 15-30 minutos
**N√≠vel:** Iniciante
**Pr√©-requisitos:** Python, documentos para indexar

## Objetivo
Aprender a selecionar e usar modelos de embedding para RAG

## O que s√£o Embeddings?
Representa√ß√µes vetoriais de texto que capturam significado sem√¢ntico:
```
Texto ‚Üí Embedding Model ‚Üí Vetor (e.g., 1536 dimens√µes)
```

Similaridade entre vetores = similaridade sem√¢ntica

## Modelos Populares

### 1. OpenAI Embeddings (Commercial)
```python
from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-large"  # ou "text-embedding-3-small"
)
```

### 2. BGE (Open Source)
```python
from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-en-v1.5"
)
```

### 3. E5 (Open Source)
```python
embeddings = HuggingFaceEmbeddings(
    model_name="microsoft/E5-large-v2"
)
```

### 4. MiniLM (Open Source - R√°pido)
```python
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
```

## Compara√ß√£o Models

| Model | Dimens√µes | Qualidade | Velocidade | Custo | Multilingue |
|-------|-----------|-----------|------------|-------|-------------|
| **text-embedding-3-large** | 3072 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | $$$ | ‚úÖ |
| **text-embedding-3-small** | 1536 | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | $ | ‚úÖ |
| **BGE-large** | 1024 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | $ | ‚úÖ |
| **E5-large** | 1024 | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | $ | ‚úÖ |
| **MiniLM** | 384 | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | $ | ‚ùå |

## Exemplo B√°sico

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma

# 1. Setup
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# 2. Embed a single text
text = "RAG √© uma t√©cnica que combina busca com gera√ß√£o"
vector = embeddings.embed_query(text)

print(f"Vector dimensions: {len(vector)}")
print(f"First 5 values: {vector[:5]}")

# 3. Embed multiple texts
texts = ["Texto 1", "Texto 2", "Texto 3"]
vectors = embeddings.embed_documents(texts)
```

## Embedding + Vector Store

```python
from langchain.document_loaders import TextLoader
from langchain.text_splitters import RecursiveCharacterTextSplitter

# 1. Load and split
loader = TextLoader("documento.txt")
docs = loader.load()
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = splitter.split_documents(docs)

# 2. Create embeddings + vector store
embeddings = HuggingFaceEmbeddings('BAAI/bge-large-en-v1.5')
vectorstore = Chroma.from_documents(chunks, embeddings)

# 3. Query
query = "O que √© RAG?"
docs = vectorstore.similarity_search(query, k=3)
print(docs[0].page_content)
```

## Sele√ß√£o de Modelos

### Use OpenAI se:
- ‚úÖ Precisa m√°xima qualidade
- ‚úÖ N√£o se importa com custo
- ‚úÖ Multilingue
- ‚úÖ Production-ready
- ‚úÖ Suporte oficial

### Use BGE se:
- ‚úÖ Quality alta + gratuito
- ‚úÖ Open source
- ‚úÖ Research/academic
- ‚úÖ Fine-tuning poss√≠vel

### Use E5 se:
- ‚úÖ Balance qualidade/velocidade
- ‚úÖ Open source
- ‚úÖ Instruction-tuned
- ‚úÖ Good general use

### Use MiniLM se:
- ‚úÖ Velocidade m√°xima
- ‚úÖ Recursos limitados
- ‚úÖ Prototipagem
- ‚úÖ Quality m√©dia ok

## C√≥digo Production-Ready

```python
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings
from langchain.embeddings.base import Embeddings
import os

class EmbeddingModel:
    def __init__(self, provider="openai", model_name=None):
        self.provider = provider
        self.model_name = model_name or self._get_default_model()
        self.model = self._load_model()

    def _get_default_model(self):
        defaults = {
            "openai": "text-embedding-3-small",
            "huggingface": "BAAI/bge-large-en-v1.5"
        }
        return defaults[self.provider]

    def _load_model(self):
        if self.provider == "openai":
            if not os.getenv("OPENAI_API_KEY"):
                raise ValueError("OPENAI_API_KEY not set")
            return OpenAIEmbeddings(model=self.model_name)
        else:
            return HuggingFaceEmbeddings(model_name=self.model_name)

    def embed_query(self, text):
        return self.model.embed_query(text)

    def embed_documents(self, texts):
        return self.model.embed_documents(texts)

# Usage
embeddings = EmbeddingModel(provider="openai")
# ou
embeddings = EmbeddingModel(provider="huggingface")
```

## Troubleshooting

### API Key Error
**Problema:** `AuthenticationError`
**Solu√ß√£o:**
```python
import os
os.environ["OPENAI_API_KEY"] = "sua-key-aqui"
```

### Model Not Found
**Problema:** `Model not found`
**Solu√ß√£o:**
```python
# Verificar nome correto
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('BAAI/bge-large-en-v1.5')
```

### Slow Embeddings
**Problema:** Muito lento
**Solu√ß√µes:**
1. Usar modelo menor (MiniLM)
2. Batch processing
3. Async embedding

### High Memory Usage
**Problema:** Out of memory
**Solu√ß√µes:**
1. Smaller model
2. Process in batches
3. Reduce batch size

## Pr√≥ximos Passos

- üíª **Exemplos Pr√°ticos:** [Code Examples](../code-examples/)
- üîß **Troubleshooting:** [Problemas Comuns](../troubleshooting/common-issues.md)
- üóÑÔ∏è **Vector DBs:** [Guia 04 - Vector Databases](../04-Vector-Databases/README.md)
